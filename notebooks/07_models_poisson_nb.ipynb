{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da9d05f-82e3-426b-a899-d80c3ee1f347",
   "metadata": {},
   "source": [
    "## Stroke Work\n",
    "<br>Author: Daniel Maina Nderitu<br>\n",
    "Project: MADIVA<br>\n",
    "Purpose: Incidence modeling<br>\n",
    "Notes:   We are comparing Poisson, robust Poisson, and NB models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e990b1-c380-44c4-a2fb-2046616a5da7",
   "metadata": {},
   "source": [
    "#### Bootstrap cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9396540-f6af-4e0d-aa61-9f0a3d470e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\n",
      "DATA_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\data\n",
      "OUT_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\model_output\n",
      "FIG_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\visualization\n",
      "NOTEBOOKS_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\notebooks\n",
      "NOTEBOOKS_EXECUTED_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\notebooks_executed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MODEL_DIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m OUT_DIR \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUT_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     54\u001b[0m FIG_DIR \u001b[38;5;241m=\u001b[39m paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFIG_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 55\u001b[0m MODEL_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMODEL_DIR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ========================================================\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'MODEL_DIR'"
     ]
    }
   ],
   "source": [
    "# =================== BOOTSTRAP CELL ===================\n",
    "# Standard setup for all notebooks\n",
    "# ========================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[0]  # assumes notebooks are in a subfolder\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# # ========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config.variables import COVARIATES\n",
    "# Import helper to load paths\n",
    "from src.utils.helpers import load_paths\n",
    "\n",
    "\n",
    "# 3Ô∏è‚É£ Load paths from config.yaml (works regardless of notebook location)\n",
    "paths = load_paths()\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "# ========================================================\n",
    "# Optional for warnings and nicer plots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ========================================================\n",
    "# 1Ô∏è‚É£ Ensure project root is in Python path\n",
    "# Adjust this if your notebooks are nested deeper\n",
    "# ========================================================\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "# ========================================================\n",
    "# 4Ô∏è‚É£ Optionally, print paths to confirm\n",
    "for key, value in paths.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# ========================================================\n",
    "# Using paths in your notebook:\n",
    "DATA_DIR = paths['DATA_DIR']\n",
    "OUT_DIR = paths['OUT_DIR']\n",
    "FIG_DIR = paths['FIG_DIR']\n",
    "MODEL_DIR = paths[\"MODEL_DIR\"]\n",
    "\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266a004-4ad7-47f9-aa07-31aacaf600ac",
   "metadata": {},
   "source": [
    "### Import data - from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23796d3c-c3f5-4649-a8a1-5e602b3ff864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Loading saved data as pickle:\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_pickle(OUT_DIR / \"df_step06_processed.pkl\")\n",
    "X = pd.read_pickle(OUT_DIR / \"X_step06_model_matrix.pkl\")\n",
    "y = pd.read_pickle(OUT_DIR / \"y_step06_event.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31442adc-0014-4490-8b2d-9eef422d9d06",
   "metadata": {},
   "source": [
    "#### prepare_pooled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15ed8d-5d09-4c6f-a073-1c730cf14ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================  \n",
    "# Prepare X, y, offset for pooled models\n",
    "# =================================================================================  \n",
    "\n",
    "def prepare_pooled_data(df, covariates, event_col='event', offset_col='offset'):\n",
    "    \"\"\"\n",
    "    Prepare data for pooled regression models, handling 'Missing' string values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select only covariates that exist in df (defensive)\n",
    "    covariates_present = [c for c in covariates if c in df.columns]\n",
    "    missing_covariates = set(covariates) - set(covariates_present)\n",
    "    \n",
    "    if missing_covariates:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Missing covariates: {missing_covariates}\")\n",
    "    print(\"‚úÖ Covariates used:\", covariates_present)\n",
    "    \n",
    "    # Create working copies\n",
    "    X_pooled = df[covariates_present].copy()\n",
    "    y = df[event_col].copy()\n",
    "    offset = df[offset_col].copy()\n",
    "    \n",
    "    # --- First: Convert 'Missing' strings to NaN ---\n",
    "    print(\"üîÑ Converting 'Missing' strings to NaN...\")\n",
    "    \n",
    "    for col in X_pooled.columns:\n",
    "        # Check if column contains string 'Missing'\n",
    "        if X_pooled[col].dtype == 'object':\n",
    "            missing_count = (X_pooled[col] == 'Missing').sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"   {col}: {missing_count} 'Missing' values ‚Üí NaN\")\n",
    "                X_pooled[col] = X_pooled[col].replace('Missing', np.nan)\n",
    "        \n",
    "        # Also check for other missing representations\n",
    "        other_missing_representations = ['missing', 'MISSING', 'Unknown', 'unknown', '']\n",
    "        if X_pooled[col].dtype == 'object':\n",
    "            for missing_val in other_missing_representations:\n",
    "                if missing_val in X_pooled[col].values:\n",
    "                    count = (X_pooled[col] == missing_val).sum()\n",
    "                    if count > 0:\n",
    "                        print(f\"   {col}: {count} '{missing_val}' values ‚Üí NaN\")\n",
    "                        X_pooled[col] = X_pooled[col].replace(missing_val, np.nan)\n",
    "    \n",
    "    # --- Now handle data types properly ---\n",
    "    print(\"üîÑ Converting data types...\")\n",
    "    \n",
    "    for col in X_pooled.columns:\n",
    "        if X_pooled[col].dtype == 'bool':\n",
    "            # Convert pure boolean to int\n",
    "            X_pooled[col] = X_pooled[col].astype(int)\n",
    "            print(f\"   {col}: bool ‚Üí int\")\n",
    "            \n",
    "        elif X_pooled[col].dtype == 'object':\n",
    "            # Try to convert to numeric, which will handle the NaN values properly\n",
    "            original_dtype = X_pooled[col].dtype\n",
    "            X_pooled[col] = pd.to_numeric(X_pooled[col], errors='coerce')\n",
    "            converted_nans = X_pooled[col].isna().sum()\n",
    "            print(f\"   {col}: {original_dtype} ‚Üí numeric ({converted_nans} NaN values)\")\n",
    "            \n",
    "        else:\n",
    "            # Ensure numeric type\n",
    "            original_dtype = X_pooled[col].dtype\n",
    "            X_pooled[col] = pd.to_numeric(X_pooled[col], errors='coerce')\n",
    "            if X_pooled[col].dtype != original_dtype:\n",
    "                print(f\"   {col}: {original_dtype} ‚Üí {X_pooled[col].dtype}\")\n",
    "    \n",
    "    # --- Check missingness percentage after conversion ---\n",
    "    check_columns = covariates_present + [event_col, offset_col]\n",
    "    missing_pct = (\n",
    "        pd.concat([X_pooled, y.rename('event'), offset.rename('offset')], axis=1)\n",
    "        .isna()\n",
    "        .mean()\n",
    "        .sort_values(ascending=False) * 100\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüîé Final percentage of missing values by variable:\")\n",
    "    for var, pct in missing_pct[missing_pct > 0].items():\n",
    "        print(f\"   {var}: {pct:.2f}%\")\n",
    "    \n",
    "    if missing_pct.max() == 0:\n",
    "        print(\"   No missing values found.\")\n",
    "    \n",
    "    # --- Drop only rows where outcome or offset are missing or invalid ---\n",
    "    valid_mask = (\n",
    "        y.notna() & \n",
    "        np.isfinite(offset) & \n",
    "        (y >= 0)  # Assuming events should be non-negative\n",
    "    )\n",
    "    \n",
    "    n_initial = len(df)\n",
    "    n_final = valid_mask.sum()\n",
    "    n_dropped = n_initial - n_final\n",
    "    \n",
    "    if n_dropped > 0:\n",
    "        print(f\"üìä Dropped {n_dropped} rows ({n_dropped/n_initial*100:.1f}%) due to missing/invalid outcomes or offsets\")\n",
    "    \n",
    "    # Apply the mask\n",
    "    X_pooled = X_pooled.loc[valid_mask]\n",
    "    y_pooled = y.loc[valid_mask]\n",
    "    offset_pooled = offset.loc[valid_mask]\n",
    "    \n",
    "    # --- Add constant term ---\n",
    "    X_pooled_const = sm.add_constant(X_pooled, has_constant='add')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final pooled dataset: {X_pooled_const.shape[0]} rows, {X_pooled_const.shape[1]} columns\")\n",
    "    print(f\"   Features: {X_pooled_const.shape[1]-1} covariates + constant\")\n",
    "    \n",
    "    return X_pooled_const, y_pooled, offset_pooled, covariates_present\n",
    "\n",
    "# =================================================================================  \n",
    "# Usage\n",
    "# =================================================================================  \n",
    "covariates_present = [c for c in COVARIATES if c in df.columns]\n",
    "print(\"Covariates used:\", covariates_present)\n",
    "\n",
    "X_pooled_const, y_pooled, offset_pooled, covariates_used = prepare_pooled_data(df, covariates_present)\n",
    "\n",
    "# =================================================================================  \n",
    "# Verify the result\n",
    "# =================================================================================  \n",
    "print(\"\\nüîç Sample of processed X_pooled:\")\n",
    "print(X_pooled_const.head())\n",
    "print(f\"\\nData types:\\n{X_pooled_const.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e390eaa-e527-40bb-aa9a-d02bbaf81b2b",
   "metadata": {},
   "source": [
    "#### Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c16b47-37fa-4393-8263-91e4a03a85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[COVARIATES + ['event', 'offset']].isna().sum()\n",
    "# np.isinf(df[covariates + ['offset']]).sum()   # Check for infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6777b-effa-4049-ad34-cf0ecfdf7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[COVARIATES + ['event', 'offset']].dropna() # dropping rows with missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98334062-bb4b-42e5-b05c-bd04bdbf1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# =================================================================================  \n",
    "X = sm.add_constant(df_clean[COVARIATES])\n",
    "y = df_clean['event']\n",
    "\n",
    "# =================================================================================  \n",
    "# =================================================================================  \n",
    "model_pois = sm.GLM(\n",
    "    y,\n",
    "    X,\n",
    "    family=sm.families.Poisson(),\n",
    "    offset=df_clean['offset']\n",
    ").fit()\n",
    "\n",
    "# =================================================================================  \n",
    "# =================================================================================  \n",
    "print(model_pois.summary())\n",
    "print(\"\\nIRR:\")\n",
    "print(np.exp(model_pois.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be216493-1344-444f-877c-22f2dff7f9a1",
   "metadata": {},
   "source": [
    "##### Poisson Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693571a-2f1a-4c24-ab7f-afaf28ab0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Print full model summary\n",
    "print(model_pois.summary())\n",
    "\n",
    "# Create a tidy summary DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Variable\": model_pois.params.index,\n",
    "    \"Coef\": model_pois.params.values,\n",
    "    \"StdErr\": model_pois.bse,\n",
    "    \"z\": model_pois.tvalues,\n",
    "    \"P>|z|\": model_pois.pvalues,\n",
    "    \"CI_lower\": model_pois.conf_int()[0],\n",
    "    \"CI_upper\": model_pois.conf_int()[1]\n",
    "})\n",
    "\n",
    "# Add Incidence Rate Ratios (IRR)\n",
    "results_df[\"IRR\"] = np.exp(results_df[\"Coef\"])\n",
    "results_df[\"IRR_CI_lower\"] = np.exp(results_df[\"CI_lower\"])\n",
    "results_df[\"IRR_CI_upper\"] = np.exp(results_df[\"CI_upper\"])\n",
    "\n",
    "# print(results_df)\n",
    "results_df.to_csv(OUT_DIR / \"poisson_model_results_main.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998880fe-50e1-4e79-b040-e0d4ffc347c2",
   "metadata": {},
   "source": [
    "#### Robust Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f595f8-d140-443a-9373-ef32ac3c6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Robust Poisson (same coefficients, larger SEs)\n",
    "model_robust = sm.GLM(y, X, family=sm.families.Poisson(), offset=df_clean[\"offset\"]).fit(cov_type='HC0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceab072-a190-4035-b9a6-3be6d3690c21",
   "metadata": {},
   "source": [
    "#### Negative Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213e84c-1ea1-491d-8558-bb9aea4e37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = sm.GLM(y, X, family=sm.families.NegativeBinomial(), offset=df_clean[\"offset\"]).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d59600-c887-4521-84ae-9d702aa3ac92",
   "metadata": {},
   "source": [
    "#### Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bc452-851f-4644-a870-b614da4e51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary_col(\n",
    "    results=[model_pois, model_robust, model_nb],\n",
    "    model_names=['Poisson', 'Robust Poisson', 'NegBinomial'],\n",
    "    stars=True,\n",
    "    float_format='%0.3f',\n",
    "    info_dict={'N':lambda x: f\"{int(x.nobs)}\"}\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09c2e3-05d1-47df-ac70-605b4933b780",
   "metadata": {},
   "source": [
    "#### IRRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6b772-7f5e-42e1-bb92-3268b776f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_irrs(model):\n",
    "    df_irr = pd.DataFrame({\n",
    "        \"Variable\": model.params.index,\n",
    "        \"Coef\": model.params.values,\n",
    "        \"StdErr\": model.bse,\n",
    "        \"z\": model.tvalues,\n",
    "        \"P>|z|\": model.pvalues,\n",
    "        \"CI_lower\": model.conf_int()[0],\n",
    "        \"CI_upper\": model.conf_int()[1]\n",
    "    })\n",
    "    df_irr[\"IRR\"] = np.exp(df_irr[\"Coef\"])\n",
    "    df_irr[\"IRR_CI_lower\"] = np.exp(df_irr[\"CI_lower\"])\n",
    "    df_irr[\"IRR_CI_upper\"] = np.exp(df_irr[\"CI_upper\"])\n",
    "    # Add significance stars\n",
    "    df_irr[\"sig\"] = df_irr[\"P>|z|\"].apply(lambda p: \n",
    "                                  \"***\" if p < 0.001 else \n",
    "                                  \"**\" if p < 0.01 else \n",
    "                                  \"*\" if p < 0.05 else \"\")\n",
    "    return df_irr\n",
    "\n",
    "# =================================================================================  \n",
    "# =================================================================================  \n",
    "\n",
    "results_pois = extract_irrs(model_pois)\n",
    "results_robust = extract_irrs(model_robust)\n",
    "results_nb = extract_irrs(model_nb)\n",
    "\n",
    "# =================================================================================  \n",
    "# --- 4Ô∏è‚É£ Combine results for export ---\n",
    "# =================================================================================  \n",
    "all_results = pd.concat([\n",
    "    results_pois.assign(Model='Poisson'),\n",
    "    results_robust.assign(Model='Robust Poisson'),\n",
    "    results_nb.assign(Model='NegBinomial')\n",
    "])\n",
    "\n",
    "# =================================================================================  \n",
    "# Save table to Excel\n",
    "# =================================================================================  \n",
    "all_results.to_excel(OUT_DIR / \"stroke_model_results_comparison_main.xlsx\", index=False)\n",
    "print(\"‚úÖ Model comparison results saved to Excel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2afca-98ba-417a-9572-db776b063dcc",
   "metadata": {},
   "source": [
    "#### Overdispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834dc20-8531-468e-b763-e2b5b84c42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = model_pois.deviance / model_pois.df_resid\n",
    "print(\"Dispersion parameter:\", dispersion)\n",
    "\n",
    "if dispersion > 1.5:\n",
    "    print(\"‚ö†Ô∏è Data likely overdispersed ‚Äî Negative Binomial model may be more appropriate.\")\n",
    "else:\n",
    "    print(\"‚úÖ Poisson model dispersion acceptable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f546146-7ae5-43d5-a0d2-aa972154f0a1",
   "metadata": {},
   "source": [
    "#### End - Saving Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05918c8b-c170-4487-8bd4-03779eb9630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as pickle (faster for large data, preserves types)\n",
    "df.to_pickle(OUT_DIR / \"df_step07_processed.pkl\")\n",
    "X.to_pickle(OUT_DIR / \"X_step07_model_matrix.pkl\")\n",
    "y.to_pickle(OUT_DIR / \"y_step07_event.pkl\")\n",
    "\n",
    "# =================================================================================  \n",
    "# Saving models\n",
    "# =================================================================================  \n",
    "import pickle\n",
    "\n",
    "with open(MODEL_DIR / \"model_pois.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_pois, f)\n",
    "\n",
    "with open(MODEL_DIR / \"model_robust.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_robust, f)\n",
    "\n",
    "with open(MODEL_DIR / \"model_nb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_nb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3a6e4-1c94-4ffd-bf04-157e2fa8d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
