{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da9d05f-82e3-426b-a899-d80c3ee1f347",
   "metadata": {},
   "source": [
    "## Stroke Work\n",
    "<br>Author: Daniel Maina Nderitu<br>\n",
    "Project: MADIVA\n",
    "Purpose: Make analysis-ready covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8465e-df1c-43da-a374-acea40ca9054",
   "metadata": {},
   "source": [
    "#### Bootstrap cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3a4cb71-4d64-4eb6-9822-bc278a6ede23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\n",
      "DATA_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\data\n",
      "OUT_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\model_output\n",
      "FIG_DIR: D:\\APHRC\\GoogleDrive_ii\\stata_do_files\\madiva\\stroke_work\\visualization\n"
     ]
    }
   ],
   "source": [
    "# =================== BOOTSTRAP CELL ===================\n",
    "# Standard setup for all notebooks\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[0]  # assumes notebooks are in a subfolder\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ========================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config.variables import COVARIATES\n",
    "from src.config.variables import KEY_PREDICTORS\n",
    "\n",
    "# ========================================================\n",
    "# Optional for warnings and nicer plots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================================\n",
    "# 1️⃣ Ensure project root is in Python path\n",
    "# Adjust this if your notebooks are nested deeper\n",
    "PROJECT_ROOT = Path.cwd().parents[0]  # assumes notebooks are in a subfolder\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ========================================================\n",
    "# 2️⃣ Import helper to load paths\n",
    "from src.utils.helpers import load_paths\n",
    "\n",
    "# ========================================================\n",
    "# 3️⃣ Load paths from config.yaml (works regardless of notebook location)\n",
    "paths = load_paths()\n",
    "\n",
    "# ========================================================\n",
    "# 4️⃣ Optionally, print paths to confirm\n",
    "for key, value in paths.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# ========================================================\n",
    "# 5️⃣ Now you can use these paths in your notebook:\n",
    "# Example:\n",
    "DATA_DIR = paths['DATA_DIR']\n",
    "OUT_DIR = paths['OUT_DIR']\n",
    "FIG_DIR = paths['FIG_DIR']\n",
    "\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e321cff-9a99-424c-8f45-a03ae6890a20",
   "metadata": {},
   "source": [
    "### Import data - from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445dadb-167d-467b-906e-c60e8cd6fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data saved as pickle:\n",
    "df = pd.read_pickle(OUT_DIR / \"df_step02_processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0ac72-510e-41c0-b77e-bd76735c6c9d",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dc673c-daca-46a4-9d2f-28e0b70b7782",
   "metadata": {},
   "source": [
    "#### Projects Name Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7df8f3b-ea62-420e-a95a-8cbafcec1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"SCALEUP Population Baseline\": \"SCALEUP_Pop_Baseline\",\n",
    "    \"Assess Linkages Main\": \"Assess_Linkages_Main\",\n",
    "    \"HAALSI_1\": \"HAALSI_1\",\n",
    "    \"HIV_NCD\": \"HIV_NCD\",\n",
    "    \"AWIGEN_1\": \"AWIGEN_1\",\n",
    "    \"HAALSI_2\": \"HAALSI_2\",\n",
    "    \"Nkateko_1\": \"Nkateko_1\",\n",
    "    \"HAALSI_3\": \"HAALSI_3\",\n",
    "    \"Nkateko_2\": \"Nkateko_2\",\n",
    "    \"ARKStudyPhase_1\": \"ARK_1\",\n",
    "    \"ARKStudyPhase_2\": \"ARK_2\",\n",
    "    \"AWIGEN_2\": \"AWIGEN_2\",\n",
    "    \"SCALEUP Clinic Baseline\": \"SCALEUP_Clinic_Baseline\",\n",
    "    \"Diabetics Baseline\": \"Diabetics_Baseline\",\n",
    "    \"Diabetics Followup\": \"Diabetics_Followup\"\n",
    "}\n",
    "\n",
    "# Rename\n",
    "df['source'] = df['source'].replace(name_map)\n",
    "\n",
    "# Make it categorical\n",
    "df['source'] = df['source'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "810e4034-b878-475a-8e5e-abc6e46472cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Nkateko_1\n",
       "1        ARK_1\n",
       "2        ARK_2\n",
       "3      HIV_NCD\n",
       "4    Nkateko_1\n",
       "Name: source, dtype: category\n",
       "Categories (15, object): ['ARK_1', 'ARK_2', 'AWIGEN_1', 'AWIGEN_2', ..., 'Nkateko_1', 'Nkateko_2', 'SCALEUP_Clinic_Baseline', 'SCALEUP_Pop_Baseline']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d47e5-0d50-48cb-be0b-23643171734b",
   "metadata": {},
   "source": [
    "#### Record type, Gender, BMI processing, and type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc8f116a-c017-4cac-9df3-13c7d8c2ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30146, 339)\n",
      "sex\n",
      "2    16391\n",
      "1    13755\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Drop individuals with only one record\n",
    "# ------------------------------------------------------------------------------------\n",
    "df = df.loc[df['record_type']==2].copy()\n",
    "print(df.shape)\n",
    "print(df.sex.value_counts())\n",
    "df['sex_binary'] = df['sex'].replace({1: 0, 2: 1})  # 0 = male, 1 = female\n",
    "df['bmi_refined'] = pd.to_numeric(df['bmi_refined'], errors='coerce')\n",
    "# ------------------------------------------------------------------------------------\n",
    "# # BMI based on WHO categories\n",
    "# ------------------------------------------------------------------------------------\n",
    "# df['bmi_category'] = pd.cut(df['bmi_refined'], \n",
    "#                             bins=[0, 18.5, 24.9, 29.9, np.inf], \n",
    "#                             labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n",
    "# Creating only two categories (making it dichotomous)\n",
    "df['bmi_category'] = pd.cut(df['bmi_refined'], \n",
    "                            bins=[0, 24.9, np.inf], \n",
    "                            labels=['Normal_Underweight', 'Overweight_Obese'])\n",
    "\n",
    "df = pd.get_dummies(df, columns=['bmi_category'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17ba8c-c4b6-4735-a965-1c882e09c970",
   "metadata": {},
   "source": [
    "#### Site & Source/study dummies (ONE-HOT ENCODING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b0e541a-fbfd-4fba-9f8d-c7d55324042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site_Nairobi']\n",
      "Site dummy variables created: ['site_Nairobi']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Making sure 'hdss_name' exists and is a string\n",
    "# ------------------------------------------------------------------------------------\n",
    "df['hdss_name'] = df['hdss_name'].astype(str)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# # Study\n",
    "# ------------------------------------------------------------------------------------\n",
    "# df['source'] = df['source'].astype('category')\n",
    "# df = pd.get_dummies(df, columns=['source'], drop_first=True)\n",
    " \n",
    "# One-hot encode site (drop one to avoid multicollinearity) - Agincourt reference\n",
    "site_dummies = pd.get_dummies(df['hdss_name'], prefix='site', drop_first=True)\n",
    "print(site_dummies.columns.tolist())\n",
    "\n",
    "# If both created, drop 'site_Agincourt' to make it reference\n",
    "if 'site_Nairobi' in site_dummies.columns:\n",
    "    if 'site_Agincourt' not in site_dummies.columns:\n",
    "        # make sure Nairobi present; handle gracefully if not\n",
    "        pass\n",
    "    site_cols = [c for c in site_dummies.columns if c != 'site_Nairobi']\n",
    "else:\n",
    "    site_cols = [c for c in site_dummies.columns]  # in case naming differs\n",
    "\n",
    "# Merge dummy columns into df\n",
    "df = pd.concat([df, site_dummies], axis=1)\n",
    "# df['site_Nairobi'] = df['site_Nairobi'].astype('category')\n",
    "# df['site_Agincourt'] = df['site_Agincourt'].astype('category')\n",
    "\n",
    "\n",
    "print(\"Site dummy variables created:\", list(site_dummies.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29c0ed-a630-4a9b-bd15-1a722af64273",
   "metadata": {},
   "source": [
    "#### Boolean conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f906c2-6915-4bfc-a5a1-6a75bb2e108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex_binary\n",
      "1    16391\n",
      "0    13755\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Convert all boolean columns to integers (0/1)\n",
    "# ------------------------------------------------------------------------------------\n",
    "df = df.astype({col: int for col in df.select_dtypes(bool).columns})\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "print(df.sex_binary.value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55765bfa-44ae-45ba-808c-77c2bb397c21",
   "metadata": {},
   "source": [
    "#### Integer conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "983f3f7b-da16-4377-8c96-f57669e7b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Converts to integer type\n",
    "# ------------------------------------------------------------------------------------\n",
    "df.bmi_category_Overweight_Obese = df.bmi_category_Overweight_Obese.astype(int)\n",
    "df['site_Nairobi'] = df['site_Nairobi'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b757f-916a-4562-8c71-0994f1c4bdf8",
   "metadata": {},
   "source": [
    "#### Date conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f58a65d5-937e-4fc6-82e4-d99ddd6cabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Convert obs_date from string (e.g., '26jul2008') to datetime\n",
    "# ------------------------------------------------------------------------------------\n",
    "df['obs_date'] = pd.to_datetime(df['obs_date'], format='%d%b%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d841a-ca72-442c-bca2-4b679f06df66",
   "metadata": {},
   "source": [
    "#### Study Periods—Start and End for Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd5fcea0-8f8e-4a26-b575-ed695bd26dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get study start and end per project (source)\n",
    "study_periods = (\n",
    "    df.groupby('source', observed=True)['obs_date']\n",
    "      .agg(study_start='min', study_end='max')\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# 2. Merge study periods back to original data\n",
    "df = df.merge(study_periods, on='source', how='left')\n",
    "\n",
    "# Create dummy variables (one-hot encode)\n",
    "df = pd.get_dummies(df, columns=['source'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb7ba3-4be4-4f54-91a8-0fc96276650c",
   "metadata": {},
   "source": [
    "#### Covariates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28659ee9-8fb2-46ea-8791-b71c5e9454b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30146, 357)\n"
     ]
    }
   ],
   "source": [
    "# covariates = ['sex_binary', 'alcohol_use', 'tobacco_use', 'hpt_status_derived', 'diab_status_derived','bmi_category_Overweight_Obese'\n",
    "#                ,'hiv_status_derived', 'site_Nairobi'] #  + list(site_dummies.columns) \n",
    "# ,'obese_status_derived' # alot of missingness\n",
    "# tb_status_derived # alot of missingness\n",
    "# , 'res_hha_wealthtertile_2.0', 'res_hha_wealthtertile_3.0'  # (Will add these once we obtain these data points)\n",
    "# ,'bmi_category_Normal', 'bmi_category_Overweight', 'bmi_category_Obese'\n",
    "# print(df.offset.describe())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998880fe-50e1-4e79-b040-e0d4ffc347c2",
   "metadata": {},
   "source": [
    "#### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30a3a6e4-1c94-4ffd-bf04-157e2fa8d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved as pickle (faster for large data, preserves types)\n",
    "df.to_pickle(OUT_DIR / \"df_step03_processed.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
